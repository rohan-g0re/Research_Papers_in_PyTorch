{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["j29BPEEi6QVl"],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Getting Started Tips\n\n1. **Start Small**: Begin with CIFAR-10 instead of ImageNet for faster iteration\n2. **Test Each Phase**: Run verification functions after implementing each phase\n3. **Debug Shapes**: Print tensor shapes frequently to catch dimension mismatches early\n4. **Use Small Batches**: Start with small batch sizes to avoid memory issues\n\nWork through each function stub systematically. The hints give you the conceptual understanding, but you'll need to research the specific PyTorch APIs and mathematical implementations. Come back with questions about specific functions when you get stuck!\n\n","metadata":{"id":"xR1XqUe26aYP"}},{"cell_type":"markdown","source":"\n# Phase 1: Environment Setup\n\n**Detailed Hint:** You need to establish your development environment with the right deep learning framework. Think about what libraries you'll need for neural networks, computer vision operations, mathematical computations, and data handling. Also consider GPU support if available. The framework choice will determine your entire implementation approach - PyTorch tends to be more research-friendly and closer to how papers describe things.\n","metadata":{"id":"b0SnUniJ5RXl"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nimport torch.nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n\nimport time\nimport os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\n\n# TODO: Fill in the necessary imports\ndef setup_environment():\n    \"\"\"\n    Set up all the necessary imports and check for GPU availability.\n    Hint: You'll need torch, torchvision, numpy, and possibly matplotlib for visualization.\n    \"\"\"\n    # Import statements go here\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.datasets as datasets\n    import torch.utils.data as data\n    from torch.utils.data import DataLoader\n    import torch.nn\n    import torch.nn.functional as F\n    from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n\n    import time\n\n\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from tqdm import tqdm\n    import random\n\n\n    # Check if CUDA is available\n    device = None  # TODO: Determine if you should use 'cuda' or 'cpu'\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n\n    print(f\"Using device: {device}\")\n    return device\n\ndef set_random_seeds(seed=42):\n    \"\"\"\n    Set random seeds for reproducibility across different libraries.\n    Hint: Neural networks involve randomness in initialization, data shuffling, etc.\n    You want consistent results across runs for debugging.\n    \"\"\"\n    # TODO: Set seeds for torch, numpy, and random module\n\n    torch.manual_seed(seed)                    # PyTorch CPU random numbers\n    torch.cuda.manual_seed(seed)               # PyTorch GPU random numbers\n    torch.cuda.manual_seed_all(seed)           # For multi-GPU setups\n    np.random.seed(seed)                       # NumPy random numbers\n    random.seed(seed)\n\n    pass\n\n\nsetup_environment()\nset_random_seeds()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhLpV_0o5Rwy","outputId":"60669a22-2086-4677-d361-bd529cc75f6b","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:31:10.167396Z","iopub.execute_input":"2025-08-06T23:31:10.167843Z","iopub.status.idle":"2025-08-06T23:31:20.434044Z","shell.execute_reply.started":"2025-08-06T23:31:10.167821Z","shell.execute_reply":"2025-08-06T23:31:20.433147Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"\n\n# Phase 2: Data Preprocessing \\& Augmentation\n\n**Detailed Hint:** AlexNet's power comes partly from its data augmentation strategy. You need to think about how to transform images during training vs testing. During training, you want randomness (different crops, flips) to artificially expand your dataset. During testing, you want consistency and thoroughness (systematic crops). The original paper mentions specific image sizes: input images are 256√ó256, but the network expects 224√ó224 patches. Consider what happens to the \"extra\" 32 pixels on each side.","metadata":{"id":"g6_X9BBp5a6G"}},{"cell_type":"code","source":"def create_basic_transforms():\n    \"\"\"\n    Create the basic image transformations for AlexNet.\n    Hint: Think about the paper's mention of 256√ó256 input images and 224√ó224 patches.\n    What mathematical operations convert images to the right format for neural networks?\n    \"\"\"\n    # TODO: Compose transformations for training\n\n    train_transform = transforms.Compose([\n\n         transforms.Resize(256),\n         transforms.RandomResizedCrop(224),\n         transforms.RandomHorizontalFlip(p=0.5),\n\n         #as 3 transformations were listed in paper, we have done all three --> now convert to tensor and move ahead\n         transforms.ToTensor(),\n\n\n         #normalization is apparrently a standard practice --> ON THE OTHER HAND,\n         # the numbers chosen are a standard practice for ImageNet\n\n         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    ])\n\n\n\n    # TODO: Compose transformations for validation/testing\n    val_transform = transforms.Compose([\n        transforms.Resize(256),\n\n        #we choose centre crop bcoz it is deterministic which is what we want in validation set -->\n        #or else each time our val acc will be different due to flips and randomk crops -->\n        #hence we also skop the flips\n\n        transforms.CenterCrop(224),\n\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n\n    return train_transform, val_transform, test_transform\n","metadata":{"id":"xigpmzwi5cmV","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:42:22.640131Z","iopub.execute_input":"2025-08-06T23:42:22.640418Z","iopub.status.idle":"2025-08-06T23:42:22.647206Z","shell.execute_reply.started":"2025-08-06T23:42:22.640393Z","shell.execute_reply":"2025-08-06T23:42:22.646300Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\ndef extract_ten_test_crops(image_tensor):\n    \"\"\"\n    Extract the 10 crops used during AlexNet testing phase.\n    Hint: The paper mentions \"four corner patches and the center patch\" plus their horizontal reflections.\n    Think about where these 5 locations would be in a 256√ó256 image when extracting 224√ó224 patches.\n    \"\"\"\n\n    \"\"\"\n    Input: image_tensor: PyTorch tensor of shape [C, H, W]\n    --> where C (Channels) =3 (RGB)\n    -->  H (Height) = W (Width) = 256\n\n    Output: list of 10 image tensors of shape [C, 224, 224]\n    \"\"\"\n    crops = []\n\n    # Verifying image_tensor shape is correct\n\n    assert image_tensor.dim() == 3, f\"Expected 3D tensor [C,W,H], got {image_tensor.dim()} D\"\n\n    assert image_tensor.shape[-2:] == (256, 256), f\"Expected 256x256 image, got with shape {image_tensor.shape[-2:]} \"\n    \n    # TODO: Extract 4 corner crops (top-left, top-right, bottom-left, bottom-right)\n\n\n# first is colon bcoz we are taking all channels (RGB)\n\n    top_left = image_tensor[:, 0:224, 0:224]\n    crops.append(top_left)\n    \n    # Top-right corner crop  \n    top_right = image_tensor[:, 0:224, 32:256]  # 256-224=32, so start at col 32\n    crops.append(top_right)\n    \n    # Bottom-left corner crop\n    bottom_left = image_tensor[:, 32:256, 0:224]  # Start at row 32\n    crops.append(bottom_left)\n    \n    # Bottom-right corner crop\n    bottom_right = image_tensor[:, 32:256, 32:256]  # Start at (32, 32)\n    crops.append(bottom_right)\n    \n    \n    \n    # TODO: Extract center crop\n\n    center = image_tensor[:, 16:240, 16:240]\n    crops.append(center)\n    \n\n\n    \n    \n    # TODO: Create horizontal flips of all 5 crops\n\n\n    #writing 5 bcoz if that is abset then at every iteration,\n    # the appended crop will also be considered --> infinite loop\n    for crop in crops[:5]: \n      \n# ---------->>>>  IMPORTANT -> torch.flip() with dims=[-1] flips along the last dimension (width)\n\n\n      flipped_crop = torch.flip(crop, dims=[-1])\n      crops.append(flipped_crop)\n\n    \n      #verif=ying if we have 10 crops or other count\n    assert len(crops) == 10, f\"Expected 10 crops, got {len(crops)}\"\n\n      # Verify all crops have correct shape\n    for i, crop in enumerate(crops):\n        assert crop.shape == (3, 224, 224), f\"Crop {i} has wrong shape: {crop.shape}\"\n    \n    return crops  # Should return list of 10 image tensors\n","metadata":{"id":"0dCkSlRcHMqo","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:43:12.556891Z","iopub.execute_input":"2025-08-06T23:43:12.557569Z","iopub.status.idle":"2025-08-06T23:43:12.567978Z","shell.execute_reply.started":"2025-08-06T23:43:12.557539Z","shell.execute_reply":"2025-08-06T23:43:12.567403Z"},"_kg_hide-input":false},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\ndef implement_pca_color_augmentation(dataset_path):\n    \"\"\"\n    Optional advanced function: Implement PCA-based color augmentation.\n    Hint: You need to collect RGB pixel values from your entire dataset,\n    compute the covariance matrix, find eigenvectors/eigenvalues,\n    then create a transform that adds random combinations of these principal components.\n    \"\"\"\n\n\n    # This is advanced - skip if you want to focus on core architecture first\n\n    # we willlc ome back later\n\n    pass","metadata":{"id":"5H0kHSrXHM2u","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:43:13.945232Z","iopub.execute_input":"2025-08-06T23:43:13.945547Z","iopub.status.idle":"2025-08-06T23:43:13.949739Z","shell.execute_reply.started":"2025-08-06T23:43:13.945525Z","shell.execute_reply":"2025-08-06T23:43:13.948983Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"\n# Phase 3: Dataset Loading\n\n**Detailed Hint:** You need to create data loaders that can efficiently feed batches of images to your network during training. Think about memory management, shuffling strategies, and how to handle different dataset formats. The original AlexNet used ImageNet, but you might start with CIFAR-10 for faster experimentation. Consider what batch size makes sense for your hardware constraints.\n","metadata":{"id":"kwvnfoGX52fk"}},{"cell_type":"code","source":"def create_data_loaders(batch_size=128):\n    \"\"\"\n    Create PyTorch DataLoaders for training and validation.\n    Hint: You need to handle the directory structure of your dataset.\n    Think about what arguments DataLoader needs for efficient training (shuffling, number of workers).\n    \"\"\"\n    train_transform, val_transform, test_transform = create_basic_transforms()\n\n    # TODO: Create dataset objects using torchvision.datasets\n    train_dataset = datasets.CIFAR10(\n        root = \"./data\",\n        train = True,\n        transform=train_transform,\n        download=True\n    )\n\n\n    val_dataset = datasets.CIFAR10(\n        root = \"./data\",\n        train = False,\n        transform=val_transform,\n        download=True\n    )\n\n\n\n    # Print dataset information\n    print(f\"Training dataset size: {len(train_dataset)} images\")\n    print(f\"Validation dataset size: {len(val_dataset)} images\")\n    print(f\"Number of classes: {len(train_dataset.classes)}\")\n    print(f\"Class names: {train_dataset.classes}\")\n\n\n\n    test_dataset = datasets.CIFAR10(\n        root=\"./data\",\n        train=False,\n        transform=test_transform,\n        download=True\n    )\n    \n\n\n\n\n    # TODO: Create DataLoader objects\n    train_loader = DataLoader(\n        dataset = train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n        drop_last=False\n    )\n\n\n\n    val_loader = DataLoader(\n        dataset = val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n        drop_last=False\n    )\n\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=32,\n        shuffle=False,\n        num_workers=2\n    )\n            \n\n    print(f\"Training batches per epoch: {len(train_loader)}\")\n    print(f\"Validation batches: {len(val_loader)}\")\n\n    return train_loader, val_loader, test_loader\n","metadata":{"id":"aQzJA93f5ajq","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:44:06.021470Z","iopub.execute_input":"2025-08-06T23:44:06.022055Z","iopub.status.idle":"2025-08-06T23:44:06.028064Z","shell.execute_reply.started":"2025-08-06T23:44:06.022036Z","shell.execute_reply":"2025-08-06T23:44:06.027343Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"\n\ndef verify_data_loading(data_loader):\n    \"\"\"\n    Test function to verify your data loading works correctly.\n    Hint: Grab a batch, check the shapes, verify the data types and value ranges.\n    Print out some statistics to ensure everything looks reasonable.\n    \"\"\"\n    # TODO: Get one batch from the data loader\n\n    data_iter = iter(data_loader)\n    images, labels = next(data_iter)\n\n\n\n\n    # Print shapes and data types\n    print(f\"Batch images shape: {images.shape}\")  # Should be [batch_size, 3, 224, 224]\n    print(f\"Batch labels shape: {labels.shape}\")  # Should be [batch_size]\n    print(f\"Images data type: {images.dtype}\")    # Should be torch.float32\n    print(f\"Labels data type: {labels.dtype}\")    # Should be torch.int64\n\n    # Check value ranges\n    print(f\"Image pixel value range: [{images.min():.3f}, {images.max():.3f}]\")\n    print(f\"Label range: [{labels.min()}, {labels.max()}]\")\n\n\n\n    # TODO: Print shapes, min/max values, data types\n    # TODO: Maybe visualize a few images to verify augmentations work\n    pass\n\n","metadata":{"id":"p-6XwcoTWBVK","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:44:09.427551Z","iopub.execute_input":"2025-08-06T23:44:09.428233Z","iopub.status.idle":"2025-08-06T23:44:09.432878Z","shell.execute_reply.started":"2025-08-06T23:44:09.428208Z","shell.execute_reply":"2025-08-06T23:44:09.432135Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"\ntrain_loader, val_loader, test_loader = create_data_loaders()\nverify_data_loading(train_loader)\nverify_data_loading(val_loader)\nverify_data_loading(test_loader)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qk65D0cxXKhb","outputId":"096e5f09-0b29-4e49-c917-52c524d95945","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:44:30.313881Z","iopub.execute_input":"2025-08-06T23:44:30.314155Z","iopub.status.idle":"2025-08-06T23:44:34.995330Z","shell.execute_reply.started":"2025-08-06T23:44:30.314129Z","shell.execute_reply":"2025-08-06T23:44:34.994596Z"}},"outputs":[{"name":"stdout","text":"Training dataset size: 50000 images\nValidation dataset size: 10000 images\nNumber of classes: 10\nClass names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nTraining batches per epoch: 391\nValidation batches: 79\nBatch images shape: torch.Size([128, 3, 224, 224])\nBatch labels shape: torch.Size([128])\nImages data type: torch.float32\nLabels data type: torch.int64\nImage pixel value range: [-2.118, 2.640]\nLabel range: [0, 9]\nBatch images shape: torch.Size([128, 3, 224, 224])\nBatch labels shape: torch.Size([128])\nImages data type: torch.float32\nLabels data type: torch.int64\nImage pixel value range: [-2.118, 2.640]\nLabel range: [0, 9]\nBatch images shape: torch.Size([32, 3, 256, 256])\nBatch labels shape: torch.Size([32])\nImages data type: torch.float32\nLabels data type: torch.int64\nImage pixel value range: [-2.118, 2.640]\nLabel range: [0, 9]\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"\n# Phase 4: Local Response Normalization (LRN)\n\n**Detailed Hint:** This is AlexNet's \"secret sauce\" for competition between feature maps. You're implementing the formula from the paper where each activation gets normalized by its neighbors. Think about how to efficiently compute the sum of squares across nearby channels for each spatial location. PyTorch removed built-in LRN, so you need to implement it as a custom module. Consider the sliding window of channels and how to handle edge cases.\n","metadata":{"id":"xU9uxsai550Y"}},{"cell_type":"code","source":"class LocalResponseNorm(nn.Module):\n    \"\"\"\n    Implement Local Response Normalization as described in AlexNet paper.\n    Hint: The formula involves looking at nearby channels and computing their squared sum.\n    You need to implement the forward pass that applies the mathematical formula to each activation.\n    \"\"\"\n\n\n    def __init__(self, size=5, alpha=1e-4, beta=0.75, k=2.0):\n        \"\"\"\n        Initialize LRN parameters.\n\n        Args:\n            size (int): Number of nearby channels to consider (n in paper)\n            alpha (float): Scaling parameter (Œ± in paper)\n            beta (float): Exponent parameter (Œ≤ in paper)\n            k (float): Additive constant to prevent division by zero\n        \"\"\"\n        super(LocalResponseNorm, self).__init__()\n        self.size = size\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n\n\n\n\n\n    def forward(self, x):\n        \"\"\"\n        Apply LRN to input tensor x.\n\n        Hint: x has shape [batch, channels, height, width]\n\n        For each position, you need to look at 'size' nearby channels,\n        compute the sum of their squares, then apply the normalization formula.\n        \"\"\"\n\n        batch_size, channels, height, width = x.size()\n\n        # TODO: Implement the LRN formula\n\n        # Step 1: Squaring the input\n\n        x_squared = x.pow(2)\n\n\n        #step 2:  Padding\n\n        #Step 2.1: Calculate how much padding we need:\n\n        padding = self.size // 2\n\n\n        # step 2.2: add the padding using the FANCY SYNTAX\n\n        x_squared_padded = F.pad(\n            x_squared,\n            (0, 0, 0, 0, padding, padding),\n            mode='constant',\n            value=0\n        )\n       # x_squared_padded shape: [batch, channels + 2*padding, height, width]\n\n\n        # Step 3: Sliding Window\n\n        # Step 3.1: Store the window states/conditions\n\n        windows = x_squared_padded.unfold(1, self.size, 1)\n\n\n        # Step 3.2: Calculate sum for each window condition\n\n        sum_of_squares = windows.sum(dim=-1)\n\n# hence we got the \"summation\" term of equation\n\n\n        denominator = torch.pow(self.k + self.alpha * sum_of_squares, self.beta)\n        denominator = torch.clamp(denominator, min=1e-8) # --> to avoid division by zero\n\n        return x / denominator  # Return normalized tensor","metadata":{"id":"4efyN8XT55kM","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:44:50.057922Z","iopub.execute_input":"2025-08-06T23:44:50.058174Z","iopub.status.idle":"2025-08-06T23:44:50.065383Z","shell.execute_reply.started":"2025-08-06T23:44:50.058154Z","shell.execute_reply":"2025-08-06T23:44:50.064451Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# Phase 5: AlexNet Architecture\n\n**Detailed Hint:** Now you're building the actual network described in the paper. Think about the sequence: convolutional layers extract features, pooling layers reduce spatial dimensions, fully connected layers make final classifications. Pay attention to the paper's specific numbers: kernel sizes, strides, number of filters, etc. The architecture has two main parts - feature extraction (convolutional) and classification (fully connected). Consider where dropout and LRN fit in the architecture.\n","metadata":{"id":"lvMG4a5r6Avj"}},{"cell_type":"code","source":"class AlexNet(nn.Module):\n    \"\"\"\n    Implement the full AlexNet architecture.\n    Hint: The paper describes 5 convolutional layers followed by 3 fully connected layers.\n    Pay attention to the specific parameters: kernel sizes, strides, padding, number of filters.\n    \"\"\"\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n\n        self.features = self._make_feature_layers()\n        self.classifier = self._make_classifier_layers(num_classes)\n\n        # TODO: Initialize weights using the strategy mentioned in the paper\n        self._initialize_weights()\n\n    def _make_feature_layers(self):\n        \"\"\"\n        Create the convolutional feature extraction layers.\n\n        Hint: Look at the paper's Table 1 or Figure 2 for the exact layer specifications.\n\n        Remember to include ReLU activations, LRN where specified, and MaxPooling layers.\n        \"\"\"\n        layers = []\n\n        # TODO: Add Conv2d, ReLU, LRN, MaxPool2d in the right sequence\n\n\n  # ----------- Layer 1: Conv(11x11, 96 filters, stride 4) -> ReLU -> LRN -> MaxPool -----------\n\n\n        # Large 11x11 kernel to capture big patterns, stride=4 to reduce size quickly\n\n\n        layers.append(nn.Conv2d(\n            in_channels=3,          # RGB input\n            out_channels=96,        # 96 different pattern detectors\n            kernel_size=11,         # 11x11 sliding window\n            stride=4,               # Move 4 pixels at a time (reduces size)\n            padding=2               # Add border to maintain reasonable size\n\n        ))\n\n\n        layers.append(nn.ReLU(inplace=True))  # Activation: keep positive values only\n\n        # Add Local Response Normalization (from your Phase 4 implementation)\n        layers.append(LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2.0))\n\n        # MaxPooling: Take the maximum in each 3x3 region, stride=2\n        layers.append(nn.MaxPool2d(kernel_size=3, stride=2))\n\n\n        # ----------- Layer 2: Conv(5x5, 256 filters, stride 1) -> ReLU -> LRN -> MaxPool  -----------\n\n\n        # Input: 96 channels -> Output: 256 feature maps\n        # Smaller 5x5 kernel for more detailed patterns\n\n\n        # We implement the FULL network on one device\n        layers.append(nn.Conv2d(96, 256, kernel_size=5, padding=2))\n        #                       ‚Üë    ‚Üë\n        #                       96   256\n        #                       ‚îÇ    ‚îî‚îÄ Total output channels\n        #                       ‚îî‚îÄ Total input channels (96, not 48) --> bcoz 48 are split across 2 GPUs according to the paper.\n\n\n        layers.append(nn.ReLU(inplace=True))\n\n        layers.append(LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2.0))\n\n        layers.append(nn.MaxPool2d(kernel_size=3, stride=2))\n\n\n\n        # ----------- Layer 3: Conv(3x3, 384 filters, stride 1) -> ReLU -----------\n\n\n        # Input: 256 channels -> Output: 384 feature maps\n        # Even smaller 3x3 kernel for fine details\n\n\n        layers.append(nn.Conv2d(256, 384, kernel_size=3, padding=1))\n        layers.append(nn.ReLU(inplace=True))\n\n\n\n        # ----------- Layer 4: Conv(3x3, 384 filters, stride 1) -> ReLU -----------\n\n        # Input: 384 channels -> Output: 384 feature maps\n        # Same size, just processing the features further\n\n        layers.append(nn.Conv2d(384, 384, kernel_size=3, padding=1))\n\n        layers.append(nn.ReLU(inplace=True))\n\n\n       # ----------- Layer 5: Conv(3x3, 256 filters, stride 1) -> ReLU -> MaxPool -----------\n\n        # Input: 384 channels -> Output: 256 feature maps\n        # Final feature extraction layer\n        layers.append(nn.Conv2d(384, 256, kernel_size=3, padding=1))\n        layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.MaxPool2d(kernel_size=3, stride=2))\n\n        return nn.Sequential(*layers)\n\n\n\n\n    def _make_classifier_layers(self, num_classes):\n        \"\"\"\n        Create the fully connected classification layers.\n        Hint: The paper mentions 3 fully connected layers with specific dimensions.\n        Don't forget dropout for regularization - where should it be applied?\n        \"\"\"\n        layers = []\n\n        # TODO: Add Linear layers with appropriate input/output dimensions\n\n        # TODO: Add ReLU activations and Dropout where appropriate\n\n        # DROPOUT: Randomly turn off 50% of neurons during training\n        # This prevents overfitting - like studying with distractions to build robustness\n        layers.append(nn.Dropout(p=0.5))\n\n\n\n        # FULLY CONNECTED LAYER 1\n        # Input: 256 * 6 * 6 = 9216 features (flattened from conv layers)\n        # Output: 4096 neurons\n        layers.append(nn.Linear(256 * 6 * 6, 4096))\n        layers.append(nn.ReLU(inplace=True))\n\n\n        # More dropout\n        layers.append(nn.Dropout(p=0.5))\n\n\n        # FULLY CONNECTED LAYER 2\n        # Input: 4096 -> Output: 4096\n        layers.append(nn.Linear(4096, 4096))\n        layers.append(nn.ReLU(inplace=True))\n\n\n\n        # Final layer should output 'num_classes' values (no activation - handled by loss function)\n\n        # Input: 4096 -> Output: num_classes (10 for CIFAR-10, 1000 for ImageNet)\n        # No activation here - the loss function (CrossEntropy) handles it\n        layers.append(nn.Linear(4096, num_classes))\n\n\n        return nn.Sequential(*layers)\n\n    def _initialize_weights(self):\n        \"\"\"\n        Initialize network weights as described in the paper.\n        Hint: The paper mentions specific initialization strategies for different layer types.\n        Conv layers and Linear layers might need different approaches.\n        \"\"\"\n\n        # we loop over every layer in the network and set weights\n        for module in self.modules():\n\n          # if layer is a conv layer\n\n            if isinstance(module, nn.Conv2d):\n                # Convolutional layers: Gaussian distribution with std=0.01\n\n                #weights\n                nn.init.normal_(module.weight, mean=0, std=0.01)\n\n                #bias\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n\n\n\n           # if layer is fc layer\n\n            elif isinstance(module, nn.Linear):\n                # Fully connected layers: Gaussian distribution with std=0.01\\\n\n              #weights\n                nn.init.normal_(module.weight, mean=0, std=0.01)\n\n                #bias\n                nn.init.constant_(module.bias, 1)  # Paper initializes FC biases to 1\n\n\n    def forward(self, x):\n\n      # self - is basically the alexnet network itself\n      # x is the input image\n\n\n\n\n        \"\"\"\n        Define the forward pass through the network.\n        Hint: Data flows through features, then gets flattened, then through classifier.\n        Pay attention to tensor shapes - when do you need to reshape?\n        \"\"\"\n        # TODO: Pass through feature extraction layers\n\n\n        #step 1: feature extraction layers\n        #pass through all conv layers\n        # Input shape: [batch_size, 3, 224, 224]\n        # Output shape: [batch_size, 256, 6, 6]\n\n\n        x = self.features(x)\n\n\n\n        '''\n        Original photo: [Cat sitting on a chair]\n\n        Detective's report (256 observations):\n        - Report 1: \"I see vertical edges in regions...\"\n        - Report 2: \"I see curved shapes in regions...\"\n        - Report 3: \"I see furry textures in regions...\"\n        - Report 4: \"I see pointy triangular shapes in regions...\"\n        - ...\n        - Report 256: \"I see whisker-like patterns in regions...\"\n\n        ----- tHEREFORE we get the shape change as ------\n\n\n        # Before feature extraction:\n        # x.shape = [batch_size, 3, 224, 224]\n        #           [how many photos, RGB, height, width]\n        #           [4, 3, 224, 224] = 4 color photos, each 224√ó224 pixels\n\n        # After feature extraction:\n        # x.shape = [batch_size, 256, 6, 6]\n        #           [how many photos, observations, small regions, small regions]\n        #           [4, 256, 6, 6] = 4 analysis reports, each with 256 observations about 6√ó6 grid\n        '''\n\n\n\n\n\n\n\n\n\n\n\n        # TODO: Flatten the tensor for fully connected layers\n        # Step 2: flatten bcoz we need to convert from 4d tensor to 2d tensore\n\n       # Think: Convert from \"image with features\" to \"list of features\"\n\n        x = x.view(x.size(0), -1) # Keep batch size, flatten everything else\n\n        # continuation from example: turning the report into a list\n\n        # New shape: [batch_size, 256*6*6] = [batch_size, 9216]\n\n        #we have done this bcoz FC layers need a list of numbers and not grids\n\n\n\n\n\n        # TODO: Step 3: Pass through classifier layers\n\n        x = self.classifier(x)\n\n        # gives us the final class label\n\n        return x\n","metadata":{"id":"OtsUJXu75bWM","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:44:54.693263Z","iopub.execute_input":"2025-08-06T23:44:54.693643Z","iopub.status.idle":"2025-08-06T23:44:54.709316Z","shell.execute_reply.started":"2025-08-06T23:44:54.693618Z","shell.execute_reply":"2025-08-06T23:44:54.708584Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"\n# Phase 6: Training Infrastructure\n\n**Detailed Hint:** You need to set up the training loop components: loss function, optimizer, and learning rate scheduling. The paper mentions specific choices - cross-entropy loss, SGD with momentum, specific learning rates and weight decay values. Think about what each hyperparameter does and why the authors chose these values. Also consider how to track and display training progress.\n","metadata":{"id":"LIb-0r_X6GYp"}},{"cell_type":"code","source":"\ndef setup_training_components(model, learning_rate=0.01):\n    \"\"\"\n    Set up loss function, optimizer, and learning rate scheduler.\n\n    Hint: AlexNet paper specifies SGD with momentum, specific weight decay values.\n\n    What loss function makes sense for multi-class classification?\n    \"\"\"\n    # TODO: Define appropriate loss function\n    criterion = nn.CrossEntropyLoss()\n\n    # TODO: Define optimizer with paper's hyperparameters\n    optimizer = optim.SGD(\n        model.parameters(),           # Which weights to update\n        lr=learning_rate,            # How big steps to take (learning rate)\n        momentum=0.9,                # How much to remember previous updates\n        weight_decay=0.0005          # Regularization to prevent overfitting\n    )\n\n\n\n    # TODO: Optional - create learning rate scheduler\n\n\n    # LEARNING RATE SCHEDULER - When to change learning speed\n    # AlexNet reduces LR by factor of 10 when validation error stops improving\n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode='min',          # Reduce LR when validation loss stops decreasing\n        factor=0.1,          # Multiply LR by 0.1 (reduce by factor of 10)\n        patience=1,         # Wait 10 epochs before reducing\n        verbose=True         # Print when LR changes\n    )\n\n    return criterion, optimizer, scheduler\n","metadata":{"id":"8S3HgFrzyuU2","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:45:52.916312Z","iopub.execute_input":"2025-08-06T23:45:52.916851Z","iopub.status.idle":"2025-08-06T23:45:52.921375Z","shell.execute_reply.started":"2025-08-06T23:45:52.916827Z","shell.execute_reply":"2025-08-06T23:45:52.920762Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"\ndef train_one_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"\n    Execute one training epoch.\n    Hint: This is your main training loop - iterate through batches,\n    compute forward pass, calculate loss, backpropagate, update weights.\n    Track metrics like loss and accuracy for monitoring.\n    \"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    start_time= time.time()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n        # TODO: Move data to appropriate device\n\n        data = data.to(device)\n        target = target.to(device)\n\n\n        # TODO: Zero gradients\n\n        optimizer.zero_grad()\n\n\n\n        # TODO: Forward pass\n\n        output = model(data)\n\n        \"\"\"\n        Example output from AlexNet:\n\n        AlexNet's guess: [0.1, 0.8, 0.05, 0.02, 0.01, 0.01, 0.005, 0.005, 0.0, 0.0]\n        Class meanings:  [plane, car, bird, cat, deer, dog, frog, horse, ship, truck]\n        Translation: \"I'm 80% confident this is a car, 10% confident it's a plane...\"\n        \"\"\"\n\n\n        # TODO: Compute loss\n\n        loss = criterion(output, target)\n\n\n# --------- STARTING BACKPROP---------\n\n        # TODO: Backward pass\n\n        loss.backward()\n\n        \"\"\"\n        computes gradients WRT all model parameters\n        - model parameteres here (i think) is the 'theta' which is basically all the weights and biases\n        \"\"\"\n\n\n\n        # TODO: Optimizer step\n\n        optimizer.step()\n\n\n\n\n        # TODO: Update running statistics\n\n        running_loss += loss.item()\n\n        predicted = torch.argmax(output, dim=1)  # Get class with highest probability\n        total += target.size(0)                  # Add batch size to total\n        correct += (predicted == target).sum().item()  # Count correct predictions\n\n\n        # Optional: Print progress every N batches\n\n        if batch_idx % 100 == 0:\n          current_acc = 100.0 * correct / total\n          print(f\"  Batch {batch_idx:4d}/{len(train_loader)}: \"\n                f\"Loss: {loss.item():.4f}, \"\n                f\"Accuracy: {current_acc:.2f}%\")\n\n\n\n\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n\n    return epoch_loss, epoch_acc\n","metadata":{"id":"LDYzb5rIywkK","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:46:12.428967Z","iopub.execute_input":"2025-08-06T23:46:12.429406Z","iopub.status.idle":"2025-08-06T23:46:12.436614Z","shell.execute_reply.started":"2025-08-06T23:46:12.429375Z","shell.execute_reply":"2025-08-06T23:46:12.435794Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"\ndef validate_model(model, val_loader, criterion, device):\n    \"\"\"\n    Evaluate model on validation set.\n    Hint: Similar to training but without gradient computation.\n    \"\"\"\n\n\n    model.eval()\n\n    \"\"\"\n    # Evaluation mode: model.eval()\n    # - Dropout is DISABLED (all neurons active for best performance)\n    # - BatchNorm uses stored statistics from training\n    # - Network gives its \"best shot\" consistently\n    \"\"\"\n\n\n\n\n    val_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad(): # as specified earlier -->  no need of calculating gradients\n\n        for data, target in val_loader:\n\n            \"\"\"\n            ALMOST SIMILAR JUS THAT -->\n            1. we are doing this for monitoring the performance of model and not to make it better\n            --> thats why we dont have a backprop process\n            --> just simple calculating loss and accuracy\n\n            2. we do all of this on DIFFERENT DATA - loaded usnig val_loader ---> NEVER SEEN BEFORE\n\n            ]\n            \"\"\"\n\n\n            \"\"\"\n            IMP ------ During validation, we want to see AlexNet's true current ability, not give it more practice!\n            \"\"\"\n\n            # TODO: Move data to device\n\n            data = data.to(device)\n            target = target.to(device)\n\n\n            # TODO: Forward pass\n\n            output = model(data)\n\n            # TODO: Compute loss\n\n            # this loss is not calculated for learning --> we are just checking/monitoring using this loss\n\n            loss = criterion(output, target)\n            val_loss += loss.item()\n\n\n            # TODO: Calculate accuracy\n\n            predicted = torch.argmax(output, dim=1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n\n    avg_loss = val_loss / len(val_loader)\n    accuracy = correct / total\n    return avg_loss, accuracy","metadata":{"id":"YlYizrYU6DrG","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:31:28.898212Z","iopub.execute_input":"2025-08-06T23:31:28.898398Z","iopub.status.idle":"2025-08-06T23:31:28.923075Z","shell.execute_reply.started":"2025-08-06T23:31:28.898383Z","shell.execute_reply":"2025-08-06T23:31:28.922526Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"\n# Phase 7: Main Training Loop\n\n**Detailed Hint:** This ties everything together - your main training script that orchestrates the entire process. Think about how many epochs to train, when to save checkpoints, how to handle early stopping, and what information to log. Consider what you want to track during training and how to save the best model.\n","metadata":{"id":"Beb02V9u6K0d"}},{"cell_type":"code","source":"# def train_alexnet(num_epochs=20, save_path=\"alexnet_checkpoint.pth\"):\n#     \"\"\"\n#     Main training function that coordinates everything.\n#     Hint: This should set up all components, then run training/validation loops.\n#     Consider saving checkpoints, tracking best performance, and logging progress.\n#     \"\"\"\n#     # TODO: Set up device, data loaders, model, training components\n#     device = setup_environment()\n#     train_loader, val_loader = create_data_loaders(batch_size = 128)\n#     model = AlexNet(num_classes=10).to(device)\n#     criterion, optimizer, scheduler = setup_training_components(model, learning_rate=0.01)\n\n#     best_val_acc = 0.0\n#     best_val_loss = float('inf')\n#     epochs_without_improvement = 0\n\n#     for epoch in range(num_epochs):\n#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n#         epoch_start_time = time.time()\n#         current_lr = optimizer.param_groups[0]['lr']\n\n\n#         # TODO: Train for one epoch\n\n#         train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n\n\n#         # TODO: Validate the model\n\n#         val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n\n\n\n#         # TODO: Update learning rate if using scheduler\n\n#         scheduler.step(val_loss)\n\n#         # Check if learning rate was reduced\n#         new_lr = optimizer.param_groups[0]['lr']\n#         if new_lr != current_lr:\n#             print(f\"üìâ Learning rate reduced: {current_lr:.6f} ‚Üí {new_lr:.6f}\")\n\n\n#         epoch_time = time.time() - epoch_start_time\n\n\n\n#         # TODO: Save checkpoint if this is the best model so far\n\n#         is_best_flag = val_acc > best_val_acc\n\n#         if is_best_flag:\n#             best_val_acc = val_acc\n#             best_val_loss = val_loss\n#             epochs_without_improvement = 0\n\n#         else:\n#             epochs_without_improvement += 1\n\n\n\n\n\n\n#         # TODO: Print/log progress\n#         print(f\"\\n EPOCH {epoch+1} SUMMARY:\")\n#         print(f\"   Time: {epoch_time:.1f}s\")\n#         print(f\"   Train ‚Üí Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n#         print(f\"   Val   ‚Üí Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n#         print(f\"   Best Val Acc: {best_val_acc:.2f}% (Epoch {epoch + 1 - epochs_without_improvement})\")\n\n\n#     print(\"Training completed!\")\n","metadata":{"id":"WGjyc2tv6Iym","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:31:28.923780Z","iopub.execute_input":"2025-08-06T23:31:28.924035Z","iopub.status.idle":"2025-08-06T23:31:28.943743Z","shell.execute_reply.started":"2025-08-06T23:31:28.924014Z","shell.execute_reply":"2025-08-06T23:31:28.943044Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_alexnet(num_epochs=2, save_path=\"alexnet_checkpoint.pth\", resume_training=True):\n    \"\"\"\n    Complete AlexNet training function with model loading/saving capability.\n    \n    Args:\n        num_epochs: Number of epochs to train\n        save_path: Path to save/load model checkpoints\n        resume_training: If True, try to load existing checkpoint\n    \n    Returns:\n        model: Trained AlexNet model\n        best_val_acc: Best validation accuracy achieved\n    \"\"\"\n    \n    print(\"üöÄ Starting AlexNet Training...\")\n    \n    # Setup environment and data\n    device = setup_environment()\n    train_loader, val_loader, _= create_data_loaders(batch_size=128)\n    model = AlexNet(num_classes=10).to(device)\n    criterion, optimizer, scheduler = setup_training_components(model, learning_rate=0.01)\n    \n    # Initialize tracking variables\n    best_val_acc = 0.0\n    best_val_loss = float('inf')\n    epochs_without_improvement = 0\n    start_epoch = 0\n    \n    # Try to load existing checkpoint if resume_training is True\n    if resume_training and os.path.exists(save_path):\n        print(f\"üì• Loading checkpoint from {save_path}...\")\n        try:\n            checkpoint = torch.load(save_path, map_location=device)\n            \n            # Load model weights\n            model.load_state_dict(checkpoint['model_state_dict'])\n            \n            # Load optimizer and scheduler states\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            \n            # Load training progress\n            start_epoch = checkpoint.get('epoch', 0)\n            best_val_acc = checkpoint.get('best_val_acc', 0.0)\n            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n            \n            print(f\"‚úÖ Checkpoint loaded successfully!\")\n            print(f\"   Resuming from epoch {start_epoch}\")\n            print(f\"   Best validation accuracy so far: {best_val_acc:.2f}%\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Error loading checkpoint: {e}\")\n            print(\"Starting fresh training...\")\n            start_epoch = 0\n            best_val_acc = 0.0\n            best_val_loss = float('inf')\n    \n    else:\n        print(\"üÜï Starting fresh training (no checkpoint found or resume_training=False)\")\n    \n    # Training loop\n    for epoch in range(start_epoch, num_epochs):\n        print(f\"\\n{'='*50}\")\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"{'='*50}\")\n        \n        epoch_start_time = time.time()\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Current learning rate: {current_lr:.6f}\")\n        \n        # Training phase\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        \n        # Validation phase\n        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n        \n        # Update learning rate\n        scheduler.step(val_loss)\n        \n        # Check if learning rate was reduced\n        new_lr = optimizer.param_groups[0]['lr']\n        if new_lr != current_lr:\n            print(f\"üìâ Learning rate reduced: {current_lr:.6f} ‚Üí {new_lr:.6f}\")\n        \n        epoch_time = time.time() - epoch_start_time\n        \n        # Check if this is the best model so far\n        is_best = val_acc > best_val_acc\n        \n        if is_best:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            epochs_without_improvement = 0\n            \n            # Save the best model checkpoint\n            checkpoint = {\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'best_val_acc': best_val_acc,\n                'best_val_loss': best_val_loss,\n                'train_acc': train_acc,\n                'train_loss': train_loss,\n                'val_acc': val_acc,\n                'val_loss': val_loss\n            }\n            \n            torch.save(checkpoint, save_path)\n            print(f\"üíæ ‚úÖ NEW BEST MODEL SAVED!\")\n            print(f\"   Validation Accuracy: {val_acc:.2f}%\")\n            print(f\"   Saved to: {save_path}\")\n            \n        else:\n            epochs_without_improvement += 1\n        \n        # Print epoch summary\n        print(f\"\\nüìä EPOCH {epoch+1} SUMMARY:\")\n        print(f\"   Time: {epoch_time:.1f}s\")\n        print(f\"   Train ‚Üí Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n        print(f\"   Val   ‚Üí Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n        print(f\"   Best Val Acc: {best_val_acc:.2f}% (achieved {epochs_without_improvement} epochs ago)\")\n        \n        # Optional: Early stopping\n        if epochs_without_improvement >= 15:  # Stop if no improvement for 15 epochs\n            print(f\"\\n‚ö†Ô∏è  EARLY STOPPING: No improvement for {epochs_without_improvement} epochs\")\n            break\n    \n    print(f\"\\nüéâ TRAINING COMPLETED!\")\n    print(f\"Best validation accuracy achieved: {best_val_acc:.2f}%\")\n    print(f\"Best model saved at: {save_path}\")\n    \n    return model, best_val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:49:15.616958Z","iopub.execute_input":"2025-08-06T23:49:15.617562Z","iopub.status.idle":"2025-08-06T23:49:15.629220Z","shell.execute_reply.started":"2025-08-06T23:49:15.617534Z","shell.execute_reply":"2025-08-06T23:49:15.628416Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"\n# if __name__ == \"__main__\":\n#     train_alexnet()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-BSGWu8YgSU","outputId":"0ae03f79-715b-4add-8fc4-0dfa0082c32c","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:49:31.305474Z","iopub.execute_input":"2025-08-06T23:49:31.306143Z","iopub.status.idle":"2025-08-06T23:49:31.309264Z","shell.execute_reply.started":"2025-08-06T23:49:31.306120Z","shell.execute_reply":"2025-08-06T23:49:31.308389Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"\n# Phase 8: Testing with 10-Crop Strategy\n\n**Detailed Hint:** Implement AlexNet's testing strategy where you extract 10 different crops from each test image and average their predictions. This is different from training where you use random crops. Think about how this averaging helps improve accuracy and robustness.\n","metadata":{"id":"j29BPEEi6QVl"}},{"cell_type":"code","source":"# def test_with_ten_crops(model, test_loader, device):\n#     \"\"\"\n#     Evaluate model using the 10-crop testing strategy from the paper.\n#     Hint: For each test image, extract 10 crops, get predictions for each,\n#     then average the softmax outputs before making final prediction.\n#     \"\"\"\n#     model.eval()\n#     correct = 0\n#     total = 0\n    \n#     with torch.no_grad():\n#         for batch_idx, (images, labels) in enumerate(test_loader):\n\n#           labels = labels.to(device)\n#           # images = images.to(device)\n#           batch_size = images.size(0)\n\n        \n#           for img in images:\n\n#             true_label = labels[i].item()\n\n\n\n#             # TODO: Extract 10 crops from this image\n#             crops = extract_ten_test_crops(img)\n            \n            \n#             # TODO: Get prediction for each crop\n#             crop_predictions = []\n#             for crop in crops:\n\n\n#               crop_batch = crop.unsqueeze_(0).to(device)  # Add batch dimension\n                \n#               # TODO: Forward pass through model\n\n#               output = model(crop_batch)\n\n\n#               # TODO: Apply softmax to get probabilities\n\n#               probabilities = F.softmax(output, dim=1)  # Convert logits to probabilities\n\n#               # Store this crop's prediction\n#               crop_predictions.append(probabilities)\n\n\n#             # STEP 3: Average the 10 predictions\n#             stacked_predictions = torch.stack(crop_predictions)\n#             avg_prediction = torch.mean(stacked_predictions, dim=0)\n\n\n#             # STEP 4: Make final classification decision\n#             predicted_class = torch.argmax(avg_prediction, dim=1).item()\n            \n#             # STEP 5: Check if prediction is correct\n#             if predicted_class == true_label:\n#                 correct += 1\n            \n#             total += 1\n\n#     final_accuracy = correct / total\n#     print(f\"10-crop test accuracy: {final_accuracy:.4f}\")\n#     return final_accuracy","metadata":{"id":"S1jggrSJ6NjF","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:49:33.609150Z","iopub.execute_input":"2025-08-06T23:49:33.609448Z","iopub.status.idle":"2025-08-06T23:49:33.613975Z","shell.execute_reply.started":"2025-08-06T23:49:33.609425Z","shell.execute_reply":"2025-08-06T23:49:33.613125Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def load_trained_model(checkpoint_path=\"alexnet_checkpoint.pth\", device=None):\n    \"\"\"\n    Load a trained AlexNet model from checkpoint.\n    \n    Args:\n        checkpoint_path: Path to the saved checkpoint\n        device: Device to load model on (if None, auto-detect)\n    \n    Returns:\n        model: Loaded AlexNet model ready for testing\n    \"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    print(f\"üì• Loading model from {checkpoint_path}...\")\n    \n    try:\n        # Create model architecture\n        model = AlexNet(num_classes=10).to(device)\n        \n        # Load checkpoint\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        \n        # Load model weights\n        model.load_state_dict(checkpoint['model_state_dict'])\n        \n        # Set to evaluation mode\n        model.eval()\n        \n        print(f\"‚úÖ Model loaded successfully!\")\n        if 'best_val_acc' in checkpoint:\n            print(f\"   Best validation accuracy: {checkpoint['best_val_acc']:.2f}%\")\n        if 'epoch' in checkpoint:\n            print(f\"   Trained for {checkpoint['epoch']} epochs\")\n        \n        return model\n        \n    except FileNotFoundError:\n        print(f\"‚ùå Error: Checkpoint file '{checkpoint_path}' not found!\")\n        return None\n    except Exception as e:\n        print(f\"‚ùå Error loading model: {e}\")\n        return None\n\n\n# Updated test function that can load model automatically\ndef test_current_model_with_ten_crops(model=None, device=None, checkpoint_path=\"alexnet_checkpoint.pth\"):\n    \"\"\"\n    Test AlexNet with 10-crop strategy. Can use provided model or load from checkpoint.\n    \n    Args:\n        model: Trained model (if None, loads from checkpoint)\n        device: Device to use (if None, auto-detect)\n        checkpoint_path: Path to checkpoint if model is None\n    \"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load model if not provided\n    if model is None:\n        print(\"No model provided, loading from checkpoint...\")\n        model = load_trained_model(checkpoint_path, device)\n        if model is None:\n            print(\"‚ùå Could not load model for testing!\")\n            return None\n    \n    print(\"üß™ Starting 10-crop testing evaluation...\")\n    \n    # # Create test data loader\n    # test_transform = transforms.Compose([\n    #     transforms.Resize(256),\n    #     transforms.ToTensor(),\n    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], \n    #                        std=[0.229, 0.224, 0.225])\n    # ])\n    \n    # test_dataset = datasets.CIFAR10(\n    #     root=\"./data\",\n    #     train=False,\n    #     transform=test_transform,\n    #     download=True\n    # )\n    \n    # test_loader = DataLoader(\n    #     dataset=test_dataset,\n    #     batch_size=32,\n    #     shuffle=False,\n    #     num_workers=2\n    # )\n\n    _, _, test_loader = create_data_loaders()\n    \n    # Test with 10-crop method\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch_idx, (images, labels) in enumerate(test_loader):\n            \n            labels = labels.to(device)\n            batch_size = images.size(0)\n            \n            # Process each image individually\n            for i in range(batch_size):\n                \n                img = images[i]  # Shape: [3, 256, 256]\n                true_label = labels[i].item()\n                \n                # Extract 10 crops\n                crops = extract_ten_test_crops(img)\n                \n                # Get predictions for all crops\n                crop_predictions = []\n                for crop in crops:\n                    crop_batch = crop.unsqueeze(0).to(device)\n                    output = model(crop_batch)\n                    probabilities = F.softmax(output, dim=1)\n                    crop_predictions.append(probabilities)\n                \n                # Average predictions\n                stacked_predictions = torch.stack(crop_predictions)\n                avg_prediction = torch.mean(stacked_predictions, dim=0)\n                predicted_class = torch.argmax(avg_prediction, dim=1).item()\n                \n                # Check accuracy\n                if predicted_class == true_label:\n                    correct += 1\n                total += 1\n            \n            # Print progress\n            if batch_idx % 20 == 0:\n                current_acc = 100.0 * correct / total if total > 0 else 0.0\n                print(f\"  Processed {batch_idx + 1} batches, Current accuracy: {current_acc:.2f}%\")\n    \n    final_accuracy = correct / total\n    print(f\"\\n‚úÖ 10-crop test accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n    return final_accuracy","metadata":{"id":"YwTUZSH36SyJ","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:52:10.219144Z","iopub.execute_input":"2025-08-06T23:52:10.219813Z","iopub.status.idle":"2025-08-06T23:52:10.231245Z","shell.execute_reply.started":"2025-08-06T23:52:10.219782Z","shell.execute_reply":"2025-08-06T23:52:10.230718Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Train your model (this will return the trained model)\ntrained_model, best_acc = train_alexnet(\n    num_epochs=2, \n    save_path=\"my_alexnet.pth\",\n    resume_training=True  # Set to False to start fresh\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:50:02.622804Z","iopub.execute_input":"2025-08-06T23:50:02.623033Z","iopub.status.idle":"2025-08-06T23:51:57.487099Z","shell.execute_reply.started":"2025-08-06T23:50:02.623017Z","shell.execute_reply":"2025-08-06T23:51:57.486302Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting AlexNet Training...\nUsing device: cuda\nTraining dataset size: 50000 images\nValidation dataset size: 10000 images\nNumber of classes: 10\nClass names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nTraining batches per epoch: 391\nValidation batches: 79\nüì• Loading checkpoint from my_alexnet.pth...\n‚úÖ Checkpoint loaded successfully!\n   Resuming from epoch 1\n   Best validation accuracy so far: 0.10%\n\n==================================================\nEpoch 2/2\n==================================================\nCurrent learning rate: 0.010000\n  Batch    0/391: Loss: 2.3018, Accuracy: 6.25%\n  Batch  100/391: Loss: 2.3011, Accuracy: 9.90%\n  Batch  200/391: Loss: 2.3043, Accuracy: 9.80%\n  Batch  300/391: Loss: 2.3009, Accuracy: 9.71%\n\nüìä EPOCH 2 SUMMARY:\n   Time: 111.1s\n   Train ‚Üí Loss: 2.3030, Accuracy: 0.10%\n   Val   ‚Üí Loss: 2.3027, Accuracy: 0.10%\n   Best Val Acc: 0.10% (achieved 1 epochs ago)\n\nüéâ TRAINING COMPLETED!\nBest validation accuracy achieved: 0.10%\nBest model saved at: my_alexnet.pth\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n\n# Test the model immediately (using the returned model)\ntest_accuracy = test_current_model_with_ten_crops(\n    model=trained_model,  # Use the returned trained model\n    device=device\n)\n\nprint(f\"üéØ Final Results:\")\nprint(f\"   Best training accuracy: {best_acc:.2f}%\")\nprint(f\"   10-crop test accuracy: {test_accuracy*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:52:22.674167Z","iopub.execute_input":"2025-08-06T23:52:22.674908Z","iopub.status.idle":"2025-08-06T23:52:40.532003Z","shell.execute_reply.started":"2025-08-06T23:52:22.674876Z","shell.execute_reply":"2025-08-06T23:52:40.530372Z"}},"outputs":[{"name":"stdout","text":"üß™ Starting 10-crop testing evaluation...\nTraining dataset size: 50000 images\nValidation dataset size: 10000 images\nNumber of classes: 10\nClass names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nTraining batches per epoch: 391\nValidation batches: 79\n  Processed 1 batches, Current accuracy: 6.25%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3811486900.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Test the model immediately (using the returned model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m test_accuracy = test_current_model_with_ten_crops(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use the returned trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3788070595.py\u001b[0m in \u001b[0;36mtest_current_model_with_ten_crops\u001b[0;34m(model, device, checkpoint_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcrop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mcrop_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                     \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0mcrop_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/4143247465.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# TODO: Step 3: Pass through classifier layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# gives us the final class label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}